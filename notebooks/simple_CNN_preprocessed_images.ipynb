{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to go the route of converting images into numpy arrays for training and testing as opposed to using the Keras built-in ImageDataGenerator and FlowFromDirectory (opting instead for a single file of raw image data and discrete file of labels).  In this notebook, I will be building a CNN from scratch an leveraing the different data pipeline methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\garrick\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "\n",
    "seed = 16\n",
    "np.random.seed(seed)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import keras.utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11205786349670335466\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1494830284\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 2206715240686807994\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#check using system GPU for processing and declaring system/GPU parameters\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for training on gpu\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# configure tensorflow before fitting model\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.99\n",
    "sess = tf.Session(config=tf_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing directory to access data (as numpy arrays)\n",
    "os.chdir('C:\\\\Users\\\\Garrick\\\\Documents\\\\Springboard\\\\Capstone Project 2\\\\datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to load data\n",
    "\n",
    "def load_array(fname):\n",
    "    return np.load(open(fname,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in labels and data (as tensors)\n",
    "\n",
    "train_labels=load_array('train_labels.npy')\n",
    "valid_labels=load_array('valid_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor=load_array('train_dataset.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_Input(X):\n",
    "    minimum=0\n",
    "    maximum=255\n",
    "    X-minimum/(maximum-minimum)\n",
    "    return X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor=Normalize_Input(train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tensor=load_array('valid_dataset.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tensor=Normalize_Input(valid_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding the training data through an Image Augmentation process (including resizing and shifting tolerance)\n",
    "\n",
    "num_classes = 120\n",
    "batch_size = 12\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "train_datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1)\n",
    "\n",
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "# note to self... perhaps the imagedatagenerator parameters I had before were root cause of low accuracy...\n",
    "\n",
    "train_generator = train_datagen.flow(x=train_tensor, y=train_labels, batch_size=batch_size, shuffle=False, seed=16)\n",
    "validation_generator = validation_datagen.flow(x=valid_tensor, y=valid_labels, batch_size=batch_size, shuffle=False, seed=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 111, 111, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 109, 109, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 54, 54, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 52, 52, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 10, 10, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 1, 2048)        67584     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 2048)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 427,236\n",
      "Trainable params: 426,654\n",
      "Non-trainable params: 582\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "wide_model_slow_learn = Sequential()\n",
    "\n",
    "wide_model_slow_learn.add(BatchNormalization(input_shape=input_shape))\n",
    "wide_model_slow_learn.add(Conv2D(64, (3, 3), strides=1, input_shape=input_shape, padding='valid', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "wide_model_slow_learn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "wide_model_slow_learn.add(BatchNormalization())\n",
    "\n",
    "wide_model_slow_learn.add(Conv2D(64, (3, 3), strides=1, activation='relu', padding='valid', kernel_constraint=maxnorm(3)))\n",
    "wide_model_slow_learn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "wide_model_slow_learn.add(BatchNormalization())\n",
    "\n",
    "wide_model_slow_learn.add(Conv2D(64, (3, 3), strides=1, activation='relu', padding='valid', kernel_constraint=maxnorm(3)))\n",
    "wide_model_slow_learn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "wide_model_slow_learn.add(BatchNormalization())\n",
    "\n",
    "wide_model_slow_learn.add(Conv2D(32, (3, 3), strides=1, activation='relu', padding='valid', kernel_constraint=maxnorm(3)))\n",
    "wide_model_slow_learn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "wide_model_slow_learn.add(BatchNormalization())\n",
    "\n",
    "wide_model_slow_learn.add(Conv2D(32, (3, 3), strides=1, activation='relu', padding='valid', kernel_constraint=maxnorm(3)))\n",
    "wide_model_slow_learn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "wide_model_slow_learn.add(BatchNormalization())\n",
    "\n",
    "wide_model_slow_learn.add(Conv2D(32, (3, 3), strides=1, activation='relu', padding='valid', kernel_constraint=maxnorm(3)))\n",
    "wide_model_slow_learn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "wide_model_slow_learn.add(BatchNormalization())\n",
    "\n",
    "wide_model_slow_learn.add(Dense(2048, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "wide_model_slow_learn.add(Dropout(0.2))\n",
    "wide_model_slow_learn.add(GlobalAveragePooling2D())\n",
    "\n",
    "wide_model_slow_learn.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "# Compile model\n",
    "\n",
    "#adam_op = Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "wide_model_slow_learn.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy']) \n",
    "#loss changed to sparse for new label data\n",
    "print(wide_model_slow_learn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.bestaugmented.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "800/800 [==============================] - 171s 214ms/step - loss: 4.8917 - acc: 0.0033 - val_loss: 4.7933 - val_acc: 0.0071\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.79325, saving model to saved_models/weights.bestaugmented.from_scratch.hdf5\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 160s 200ms/step - loss: 4.8181 - acc: 0.0034 - val_loss: 4.7950 - val_acc: 0.0083\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 167s 209ms/step - loss: 4.8077 - acc: 0.0029 - val_loss: 4.7906 - val_acc: 0.0104\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.79325 to 4.79056, saving model to saved_models/weights.bestaugmented.from_scratch.hdf5\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 160s 200ms/step - loss: 4.7987 - acc: 0.0041 - val_loss: 4.7889 - val_acc: 0.0088\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.79056 to 4.78889, saving model to saved_models/weights.bestaugmented.from_scratch.hdf5\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 208s 260ms/step - loss: 4.7970 - acc: 0.0023 - val_loss: 4.7892 - val_acc: 0.0083\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 161s 201ms/step - loss: 4.7972 - acc: 0.0023 - val_loss: 4.8296 - val_acc: 0.0083\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x239905b4b00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_model_slow_learn.fit_generator(train_generator, validation_data=validation_generator,\n",
    "                         steps_per_epoch=800, epochs=10, callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase batch size, less parameters on ImageDataGenerator\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow(x=train_tensor, y=train_labels, batch_size=batch_size, shuffle=False, seed=16)\n",
    "validation_generator = validation_datagen.flow(x=valid_tensor, y=valid_labels, batch_size=batch_size, shuffle=False, seed=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "480/480 [==============================] - 101s 211ms/step - loss: 4.7939 - acc: 0.0047 - val_loss: 4.8078 - val_acc: 0.0042\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 101s 210ms/step - loss: 4.7984 - acc: 0.0040 - val_loss: 4.7918 - val_acc: 0.0079\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 103s 214ms/step - loss: 4.7941 - acc: 0.0045 - val_loss: 4.7896 - val_acc: 0.0079\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 110s 230ms/step - loss: 4.7911 - acc: 0.0061 - val_loss: 4.7926 - val_acc: 0.0054\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 104s 216ms/step - loss: 4.7962 - acc: 0.0033 - val_loss: 4.8216 - val_acc: 0.0075\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23bbf64d710>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_model_slow_learn.fit_generator(train_generator, validation_data=validation_generator,\n",
    "                         steps_per_epoch=480, epochs=10, callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_8 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 111, 111, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 109, 109, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 54, 54, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 52, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 10, 10, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               30840     \n",
      "=================================================================\n",
      "Total params: 425,444\n",
      "Trainable params: 424,446\n",
      "Non-trainable params: 998\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# trying new model with increase # of filters and \"he_normal\" kernel initializer.  \"glorot_uniform\" is default\n",
    "\n",
    "new_model = Sequential()\n",
    "\n",
    "new_model.add(BatchNormalization(input_shape=input_shape))\n",
    "new_model.add(Conv2D(16, (3, 3), strides=1, kernel_initializer='he_normal', activation='relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "new_model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "new_model.add(Conv2D(32, (3, 3), strides=1, kernel_initializer='he_normal', activation='relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "new_model.add(BatchNormalization())\n",
    "\n",
    "new_model.add(Conv2D(64, (3, 3), strides=1, kernel_initializer='he_normal', activation='relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "new_model.add(BatchNormalization())\n",
    "\n",
    "new_model.add(Conv2D(128, (3, 3), strides=1, kernel_initializer='he_normal', activation='relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "new_model.add(BatchNormalization())\n",
    "\n",
    "new_model.add(Conv2D(256, (3, 3), strides=1, kernel_initializer='he_normal', activation='relu'))\n",
    "new_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "new_model.add(BatchNormalization())\n",
    "\n",
    "new_model.add(GlobalAveragePooling2D())\n",
    "\n",
    "new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "new_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "480/480 [==============================] - 91s 189ms/step - loss: 4.8329 - acc: 0.0108 - val_loss: 4.8078 - val_acc: 0.0079\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 90s 187ms/step - loss: 4.8269 - acc: 0.0069 - val_loss: 4.7867 - val_acc: 0.0083\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.78889 to 4.78671, saving model to saved_models/weights.bestaugmented.from_scratch.hdf5\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 89s 185ms/step - loss: 4.8236 - acc: 0.0068 - val_loss: 4.8822 - val_acc: 0.0100\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 89s 186ms/step - loss: 4.8192 - acc: 0.0082 - val_loss: 4.7907 - val_acc: 0.0075\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23bbf8dd7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit_generator(train_generator, validation_data=validation_generator,\n",
    "                         steps_per_epoch=480, epochs=10, callbacks=[checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
